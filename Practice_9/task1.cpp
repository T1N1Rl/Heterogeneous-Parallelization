#include <mpi.h>    
#include <iostream> 
#include <vector>   
#include <random>   
#include <cmath>    


// Задание 1: Распределённое вычисление среднего и стандартного отклонения
int main(int argc, char** argv) {
    // Инициализация среды MPI. Принимает аргументы командной строки. Должна быть первой командой.
    MPI_Init(&argc, &argv); 

    int rank = 0; // Переменная для хранения уникального номера (ранга) текущего процесса
    int size = 1; // Переменная для хранения общего количества запущенных процессов

    // Получаем номер текущего процесса и записываем в rank (от 0 до size-1)
    MPI_Comm_rank(MPI_COMM_WORLD, &rank); 
    // Получаем общее количество процессов в коммуникаторе и записываем в size
    MPI_Comm_size(MPI_COMM_WORLD, &size); 

    int N = 1000000; // Устанавливаем размер массива по умолчанию (миллион элементов)
    
    if (argc >= 2) {
        N = std::atoi(argv[1]); // Преобразуем строку аргумента в целое число
        if (N <= 0) N = 1000000; // Если число некорректно, возвращаемся к значению по умолчанию
    }

    std::vector<double> data; // Объявляем вектор для хранения исходных данных
    // Только процесс с рангом 0 (главный) занимается генерацией исходных данных
    if (rank == 0) {
        data.resize(N); // Выделяем память под N элементов типа double
        std::mt19937 gen(42); // Инициализируем генератор Вихрь Мерсенна зерном 42 для воспроизводимости
        std::uniform_real_distribution<double> dist(0.0, 1.0); // Определяем равномерное распределение от 0 до 1
        for (int i = 0; i < N; ++i) {
            data[i] = dist(gen); // Заполняем массив случайными числами
        }
    }

    // Подготовка к распределению массива между процессами
    // sendcounts хранит количество элементов для каждого процесса, displs — смещения (отступы)
    std::vector<int> sendcounts(size), displs(size);
    int base = N / size; // Базовое количество элементов (целая часть деления N на количество процессов)
    int rem  = N % size; // Остаток от деления (лишние элементы, которые нужно распределить)

    int offset = 0; // Переменная для подсчета текущего смещения в исходном массиве
    for (int r = 0; r < size; ++r) {
        // Если номер процесса меньше остатка, даем ему на 1 элемент больше
        int cnt = base + (r < rem ? 1 : 0); 
        sendcounts[r] = cnt; // Записываем, сколько элементов отправить процессу r
        displs[r] = offset;  // Записываем, с какого индекса начинать чтение для процесса r
        offset += cnt;       // Увеличиваем смещение на количество обработанных элементов
    }

    int local_n = sendcounts[rank]; // Определяем количество элементов, которые получит именно текущий процесс
    std::vector<double> local_data(local_n); // Выделяем память под локальную часть массива

    // Барьерная синхронизация: все процессы ждут здесь, пока каждый не дойдет до этой строки
    MPI_Barrier(MPI_COMM_WORLD); 
    // Засекаем время начала выполнения распределенной части с помощью функции MPI
    double t_start = MPI_Wtime(); 

    // Распределяем данные из процесса 0 по всем остальным процессам
    // 
    MPI_Scatterv(
        rank == 0 ? data.data() : nullptr, // Адрес исходного массива (только для процесса-отправителя)
        sendcounts.data(),                 // Массив с количествами отправляемых элементов
        displs.data(),                     // Массив со смещениями элементов
        MPI_DOUBLE,                        // Тип данных в исходном массиве
        local_data.data(),                 // Адрес буфера приема (куда записывать на каждом процессе)
        local_n,                           // Количество принимаемых элементов
        MPI_DOUBLE,                        // Тип принимаемых данных
        0,                                 // Номер процесса-отправителя (root)
        MPI_COMM_WORLD                     // Коммуникатор (группа процессов)
    );

    // Локальные вычисления: каждый процесс обрабатывает только свою часть данных
    double local_sum = 0.0;     // Переменная для суммы элементов в локальном блоке
    double local_sum_sq = 0.0;  // Переменная для суммы квадратов в локальном блоке
    for (int i = 0; i < local_n; ++i) {
        double x = local_data[i]; // Текущий элемент
        local_sum    += x;        // Накапливаем сумму
        local_sum_sq += x * x;    // Накапливаем сумму квадратов
    }

    // Собираем локальные суммы со всех процессов в итоговую переменную на процессе 0
    double global_sum = 0.0;    // Переменная для общей суммы всех N элементов
    double global_sum_sq = 0.0; // Переменная для общей суммы квадратов всех N элементов

    // Складываем все local_sum в global_sum (операция MPI_SUM) и отправляем на процесс 0
    // 
    MPI_Reduce(&local_sum,    &global_sum,    1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    // Аналогично складываем все суммы квадратов
    MPI_Reduce(&local_sum_sq, &global_sum_sq, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    // Засекаем время окончания работы алгоритма
    double t_end = MPI_Wtime(); 

    // Финальные расчеты делает только главный процесс
    if (rank == 0) {
        // Вычисляем среднее арифметическое (сумма / N)
        double mean = global_sum / static_cast<double>(N); 
        // Вычисляем дисперсию (сумма_квадратов / N - среднее^2)
        double var  = global_sum_sq / static_cast<double>(N) - mean * mean; 
        // Если из-за погрешности double число чуть меньше 0, исправляем
        if (var < 0.0) var = 0.0; 
        // Вычисляем стандартное отклонение (корень из дисперсии)
        double stddev = std::sqrt(var); 

        // Выводим результаты в консоль
        std::cout << "=== Task 1: Mean & Std (N=" << N << ", P=" << size << ") ===\n";
        std::cout << "Mean     = " << mean << "\n";
        std::cout << "Std dev  = " << stddev << "\n";
        std::cout << "Exec time= " << (t_end - t_start) << " seconds\n";
    }

    // Завершаем работу MPI, очищаем ресурсы
    MPI_Finalize(); 
    return 0; // Завершение программы
}